groq:
  enabled: true
  priority: 1
  rate_limits:
    requests_per_minute: 30
    tokens_per_minute: 14400
  models:
  - openai/gpt-oss-120b
  - llama-3.3-70b-versatile
  - llama-3.1-8b-instant
  default_model: openai/gpt-oss-120b
ollama_cloud:
  enabled: true
  priority: 2
  rate_limits:
    requests_per_minute: 60
    tokens_per_minute: 50000
  models:
  - deepseek-v3.2
  - deepseek-v3.1:671b
  - glm-5
  - kimi-k2.5
  - cogito-2.1:671b
  - mistral-large-3:675b
  - qwen3-coder:480b
  - qwen3-coder-next
  - qwen3-next:80b
  - gpt-oss:120b
  - gemma3:27b
  - nemotron-3-nano:30b
  - devstral-small-2:24b
  - ministral-3:14b
  - rnj-1:8b
  - glm-4.7-flash
  - lfm2.5-thinking
  - translategemma
  default_model: deepseek-v3.2
ollama_local:
  enabled: false
  priority: 3
  rate_limits:
    requests_per_minute: 100
    tokens_per_minute: 100000
  models: []
  default_model: llama3.1
